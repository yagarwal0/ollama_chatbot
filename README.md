With this code, we can use the locally deployed Ollama Model Lamma3.2 to chat

Step to use it.
* Firstly, download the ollama from its official website
* In the CMD type the command `ollama pull llama3.2` to download the llama 3.2
* Open VS Code
* Type the above code
* Install the Streamlit library using command `pip install streamlit`
* Paste the code into the code editor
* Run the app using the command `streamlit python main.py`

A window will open in your default browser where you can ask some query from the model, and an answer will be returned.
